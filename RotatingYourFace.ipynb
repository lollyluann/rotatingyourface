{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RotatingYourFace.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "580e4b979a834847a94df049ba31cd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6ab277b2a0b44cc684305bbab65a936f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_268a2f4d4550412280a5402a3ca34b34",
              "IPY_MODEL_530dabb2318f4ebda5a25779fc090c00"
            ]
          }
        },
        "6ab277b2a0b44cc684305bbab65a936f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "268a2f4d4550412280a5402a3ca34b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4c1fb6e19a3c4e32839bbb356107373b",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7479183ace64d7992fcd261b5726ff5"
          }
        },
        "530dabb2318f4ebda5a25779fc090c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd181682e7364ed4a6a935c23ed6806c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/5 [00:02&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d43e6b0c72f34feeb29552ad5e40345e"
          }
        },
        "4c1fb6e19a3c4e32839bbb356107373b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7479183ace64d7992fcd261b5726ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd181682e7364ed4a6a935c23ed6806c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d43e6b0c72f34feeb29552ad5e40345e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UmKQrLcRTVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fc13de-4c6d-4109-c6fa-52240ab8256c"
      },
      "source": [
        "!mkdir data\n",
        "!unzip 10imgs.zip -d data/images/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  10imgs.zip\n",
            "  inflating: data/images/0.jpg       \n",
            "  inflating: data/images/__MACOSX/._0.jpg  \n",
            "  inflating: data/images/1.jpg       \n",
            "  inflating: data/images/__MACOSX/._1.jpg  \n",
            "  inflating: data/images/2.jpg       \n",
            "  inflating: data/images/__MACOSX/._2.jpg  \n",
            "  inflating: data/images/3.jpg       \n",
            "  inflating: data/images/__MACOSX/._3.jpg  \n",
            "  inflating: data/images/4.jpg       \n",
            "  inflating: data/images/__MACOSX/._4.jpg  \n",
            "  inflating: data/images/5.jpg       \n",
            "  inflating: data/images/__MACOSX/._5.jpg  \n",
            "  inflating: data/images/6.jpg       \n",
            "  inflating: data/images/__MACOSX/._6.jpg  \n",
            "  inflating: data/images/7.jpg       \n",
            "  inflating: data/images/__MACOSX/._7.jpg  \n",
            "  inflating: data/images/8.jpg       \n",
            "  inflating: data/images/__MACOSX/._8.jpg  \n",
            "  inflating: data/images/9.jpg       \n",
            "  inflating: data/images/__MACOSX/._9.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CoiJcM1GBOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2c44ca-e76f-4131-88d2-943703fdbd58"
      },
      "source": [
        "!pip install tensorflow_io"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_io\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/d2/6fd39a3519e325037462721092248b468ccbeeeb5dc870cea072655ee4b0/tensorflow_io-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1MB 125kB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/37/6cedfcc52f1d53a79a60204fc89d1f7ca099c5d3a999d4640a2fe407e91b/tensorflow_io_gcs_filesystem-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 45.2MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.6.0,>=2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/fd/993aa1333eb54d9f000863fe8ec61e41d12eb833dea51484c76c038718b5/tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.0)\n",
            "Collecting h5py~=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/74/9eae2bedd8201ab464308f42c601a12d79727a1c87f0c867fdefb212c6cf/h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 36.7MB/s \n",
            "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/e7/53bc896aa4e11a87aac10a625c676b3a3d57d1c8d9929e4809d31fa0b7d5/keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.12.4)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.19.5)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/78/b27f73e923becc6e79e18fe112cf75e3200d1ee35b0dba8fa46181bce56c/tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 46.0MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.34.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/d1/f38a91d8724706427fe973a7dfa11e938cee98aa7196b03d870a25a08bab/grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.7.4.3)\n",
            "Collecting tensorboard~=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.12.0)\n",
            "Collecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (56.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.8.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.0.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.4.1)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, cached-property, h5py, keras-nightly, gast, tensorflow-estimator, grpcio, tensorboard-data-server, tensorboard, tensorflow, tensorflow-io\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed cached-property-1.5.2 gast-0.4.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorflow-2.5.0 tensorflow-estimator-2.5.0 tensorflow-io-0.18.0 tensorflow-io-gcs-filesystem-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBWffh077Z6P",
        "outputId": "1138d1a3-96ed-4bc4-cd40-a45b5941d642"
      },
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\r\u001b[K     |██                              | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 19.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 71kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 81kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.6)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.19.5)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyOdQqlbW7VR"
      },
      "source": [
        "import os, time, random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQRXqiAE9GKI",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''\n",
        "    implement Light CNN\n",
        "    @author: Alfred Xiang Wu\n",
        "    @date: 2017.07.04\n",
        "'''\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class mfm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, type=1):\n",
        "        super(mfm, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        if type == 1:\n",
        "            self.filter = nn.Conv2d(in_channels, 2*out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        else:\n",
        "            self.filter = nn.Linear(in_channels, 2*out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.filter(x)\n",
        "        out = torch.split(x, self.out_channels, 1)\n",
        "        return torch.max(out[0], out[1])\n",
        "\n",
        "class group(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(group, self).__init__()\n",
        "        self.conv_a = mfm(in_channels, in_channels, 1, 1, 0)\n",
        "        self.conv   = mfm(in_channels, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_a(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class resblock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(resblock, self).__init__()\n",
        "        self.conv1 = mfm(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = mfm(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = out + res\n",
        "        return out\n",
        "\n",
        "class network_29layers_v2(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=80013):\n",
        "        super(network_29layers_v2, self).__init__()\n",
        "        self.conv1    = mfm(1, 48, 5, 1, 2)\n",
        "        self.block1   = self._make_layer(block, layers[0], 48, 48)\n",
        "        self.group1   = group(48, 96, 3, 1, 1)\n",
        "        self.block2   = self._make_layer(block, layers[1], 96, 96)\n",
        "        self.group2   = group(96, 192, 3, 1, 1)\n",
        "        self.block3   = self._make_layer(block, layers[2], 192, 192)\n",
        "        self.group3   = group(192, 128, 3, 1, 1)\n",
        "        self.block4   = self._make_layer(block, layers[3], 128, 128)\n",
        "        self.group4   = group(128, 128, 3, 1, 1)\n",
        "        self.fc       = nn.Linear(8*8*128, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes, bias=False)\n",
        "            \n",
        "    def _make_layer(self, block, num_blocks, in_channels, out_channels):\n",
        "        layers = []\n",
        "        for i in range(0, num_blocks):\n",
        "            layers.append(block(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.group1(x)\n",
        "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
        "\n",
        "        x = self.block2(x)\n",
        "        x = self.group2(x)\n",
        "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
        "\n",
        "        x = self.block3(x)\n",
        "        x = self.group3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.group4(x)\n",
        "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        fc = self.fc(x)\n",
        "        x = F.dropout(fc, training=self.training)\n",
        "        out = self.fc2(x)\n",
        "        return out, fc\n",
        "\n",
        "def LightCNN_29Layers_v2(**kwargs):\n",
        "    model = network_29layers_v2(resblock, [1, 2, 3, 4], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB3TBwjzyEcc"
      },
      "source": [
        "###### INPUT PARAMETERS #######\n",
        "\n",
        "data_dir = \"data/\" # images should be inside a subdirectory within the directory listed here\n",
        "img_size = (60,60)\n",
        "batch_size = 2\n",
        "num_tasks = 2 #put 2 for 2-task network and 3 for 3-task network"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp1VaGMnxy1b",
        "outputId": "27621588-c7ea-4825-cf0a-14eb7044ccff"
      },
      "source": [
        "# yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels)\n",
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0,\n",
        "  seed=6869,\n",
        "  label_mode = None,\n",
        "  shuffle = True,\n",
        "  #color_mode = \"grayscale\",\n",
        "  image_size=img_size,\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_zkE_L4Ipuq"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model, layers\n",
        "\n",
        "from models.hrnet import HRNetBody, hrnet_body\n",
        "\n",
        "\n",
        "def hrnet_stem(filters=64):\n",
        "    stem_layers = [layers.Conv2D(filters, 3, 2, 'same'),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Conv2D(filters, 3, 2, 'same'),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Activation('relu')]\n",
        "\n",
        "    def forward(x):\n",
        "        for layer in stem_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    return forward\n",
        "\n",
        "\n",
        "def hrnet_heads(input_channels=64, output_channels=17):\n",
        "    # Construct up sacling layers.\n",
        "    scales = [2, 4, 8]\n",
        "    up_scale_layers = [layers.UpSampling2D((s, s)) for s in scales]\n",
        "    concatenate_layer = layers.Concatenate(axis=3)\n",
        "    heads_layers = [layers.Conv2D(filters=input_channels, kernel_size=(1, 1),\n",
        "                                  strides=(1, 1), padding='same'),\n",
        "                    layers.BatchNormalization(),\n",
        "                    layers.Activation('relu'),\n",
        "                    layers.Conv2D(filters=output_channels, kernel_size=(1, 1),\n",
        "                                  strides=(1, 1), padding='same')]\n",
        "\n",
        "    def forward(inputs):\n",
        "        scaled = [f(x) for f, x in zip(up_scale_layers, inputs[1:])]\n",
        "        x = concatenate_layer([inputs[0], scaled[0], scaled[1], scaled[2]])\n",
        "        for layer in heads_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    return forward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3tS7XR6MV_F"
      },
      "source": [
        "def hrnet_v2(input_shape, output_channels, width=18, name=\"hrnetv2\"):\n",
        "    \"\"\"This function returns a functional model of HRNetV2.\n",
        "    Args:\n",
        "        width: the hyperparameter width.\n",
        "        output_channels: number of output channels.\n",
        "    Returns:\n",
        "        a functional model.\n",
        "    \"\"\"\n",
        "    # Get the output size of the HRNet body.\n",
        "    last_stage_width = sum([width * pow(2, n) for n in range(4)])\n",
        "\n",
        "    # Describe the model.\n",
        "    inputs = keras.Input(input_shape, dtype=tf.float32)\n",
        "    x = hrnet_stem(64)(inputs)\n",
        "    x = hrnet_body(width)(x)\n",
        "    outputs = hrnet_heads(input_channels=last_stage_width,\n",
        "                          output_channels=output_channels)(x)\n",
        "\n",
        "    # Construct the model and return it.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(0.001, amsgrad=True, epsilon=0.001),\n",
        "                  loss=keras.losses.MeanSquaredError(),\n",
        "                  metrics=[keras.metrics.MeanSquaredError()])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0BlrRHVMjoa"
      },
      "source": [
        "class YourFaceModel(tf.keras.Model):\n",
        "  def __init__(self, img_size):\n",
        "    super(YourFaceModel, self).__init__()\n",
        "    \n",
        "    lc1 = layers.LocallyConnected2D(32, (7,7), strides=(1, 1), activation=tf.nn.relu)\n",
        "    mp1 = layers.MaxPool2D(pool_size=(2, 2), strides=None)\n",
        "    fl1 = layers.Reshape((1, 1, -1))\n",
        "    fc1 = layers.Dense(3600, activation=tf.nn.relu)\n",
        "    fl2 = layers.Reshape((60, 60, 1))\n",
        "    lc2 = layers.LocallyConnected2D(32, (5,5), strides=(1, 1), activation=tf.nn.relu)\n",
        "    mp2 = layers.MaxPool2D(pool_size=(3, 3), strides=None)\n",
        "    fl3 = layers.Reshape((1, 1, -1))\n",
        "    fc2 = layers.Dense(3721, activation=tf.nn.relu)\n",
        "    fl4 = layers.Reshape((61, 61, 1)) \n",
        "\n",
        "    # task 2\n",
        "    fl45 = layers.Reshape((1, 1, -1))\n",
        "    fc3 = layers.Dense(3600, activation=tf.nn.relu)\n",
        "    fl5 = layers.Reshape((60, 60, 1))\n",
        "    lc3 = layers.LocallyConnected2D(32, (5,5), strides=(1, 1), activation=tf.nn.relu)\n",
        "    mp3 = layers.MaxPool2D(pool_size=(3, 3), strides=None)\n",
        "    fl6 = layers.Reshape((1, 1, -1))\n",
        "    fc4 = layers.Dense(3721, activation=tf.nn.relu)\n",
        "    fl7 = layers.Reshape((61, 61, 1))\n",
        "\n",
        "    task_layers = [lc1, mp1, fl1, fc1, fl2, lc2, mp2, fl3, fc2, fl4]\n",
        "    self.first_task = keras.Sequential(task_layers)\n",
        "\n",
        "    auxiliary_layers = [fl45, fc3, fl5, lc3, mp3, fl6, fc4, fl7]\n",
        "    self.aux_task = keras.Sequential(auxiliary_layers)\n",
        "\n",
        "    self.first_task.build((batch_size, img_size[0], img_size[1], 3))\n",
        "    self.aux_task.build((batch_size, img_size[0], img_size[1], 1))\n",
        "\n",
        "    self.keypoint_model = hrnet_v2(input_shape=(batch_size, img_size[0], img_size[1], 1), \n",
        "                                  output_channels=98, width=18, name=\"hrnetv2\")\n",
        "    self.keypoint_model.build((batch_size, img_size[0], img_size[1], 1))\n",
        "    #self.keypoint_model.compile(optimizer=keras.optimizers.Adam(0.001, amsgrad=True, epsilon=0.001),\n",
        "    #              loss=keras.losses.MeanSquaredError(),\n",
        "    #              metrics=[keras.metrics.MeanSquaredError()])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.first_task(inputs)\n",
        "    x2 = self.aux_task(x)\n",
        "    return x, x2, self.keypoint_model(x2)\n",
        "\n",
        "if num_tasks==3:\n",
        "  model = YourFaceModel(img_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bckfbkniXnSA"
      },
      "source": [
        "if num_tasks==2:\n",
        "  # task 1\n",
        "  lc1 = layers.LocallyConnected2D(32, (7,7), strides=(1, 1), activation=tf.nn.relu)\n",
        "  mp1 = layers.MaxPool2D(pool_size=(2, 2), strides=None)\n",
        "  fl1 = layers.Reshape((1, 1, -1))\n",
        "  fc1 = layers.Dense(3600, activation=tf.nn.relu)\n",
        "  fl2 = layers.Reshape((60, 60, 1))\n",
        "  lc2 = layers.LocallyConnected2D(32, (5,5), strides=(1, 1), activation=tf.nn.relu)\n",
        "  mp2 = layers.MaxPool2D(pool_size=(3, 3), strides=None)\n",
        "  fl3 = layers.Reshape((1, 1, -1))\n",
        "  fc2 = layers.Dense(3721, activation=tf.nn.relu)\n",
        "  fl4 = layers.Reshape((61, 61, 1)) \n",
        "\n",
        "  # task 2\n",
        "  fl45 = layers.Reshape((1, 1, -1))\n",
        "  fc3 = layers.Dense(3600, activation=tf.nn.relu)\n",
        "  fl5 = layers.Reshape((60, 60, 1))\n",
        "  lc3 = layers.LocallyConnected2D(32, (5,5), strides=(1, 1), activation=tf.nn.relu)\n",
        "  mp3 = layers.MaxPool2D(pool_size=(3, 3), strides=None)\n",
        "  fl6 = layers.Reshape((1, 1, -1))\n",
        "  fc4 = layers.Dense(3721, activation=tf.nn.relu)\n",
        "  fl7 = layers.Reshape((61, 61, 1))\n",
        "\n",
        "  task_layers = [lc1, mp1, fl1, fc1, fl2, lc2, mp2, fl3, fc2, fl4]\n",
        "  auxiliary_layers = [fl45, fc3, fl5, lc3, mp3, fl6, fc4, fl7]\n",
        "\n",
        "  task_layers.extend(auxiliary_layers)\n",
        "\n",
        "  model = keras.Sequential(task_layers)\n",
        "  model.build((batch_size, img_size[0], img_size[1], 3))\n",
        "\n",
        "  first_task = keras.Model(inputs=model.inputs,\n",
        "                          outputs=model.layers[9].output)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-3o_LRt8X2c"
      },
      "source": [
        "def light_CNN():\n",
        "  model = LightCNN_29Layers_v2()\n",
        "  checkpoint = torch.load('LightCNN_29Layers_V2_checkpoint.pth.tar')\n",
        "\n",
        "  state_dict=OrderedDict()\n",
        "\n",
        "  for (key, value) in checkpoint['state_dict'].items():\n",
        "    new_key = '.'.join(key.split('.')[1:])\n",
        "    state_dict[new_key] = value\n",
        "\n",
        "  model.load_state_dict(state_dict)\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTuJp643RUM5"
      },
      "source": [
        "def kp_model(model_dir):\n",
        "  model = tf.keras.models.load_model(model_dir)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX1fhhdW6rSn"
      },
      "source": [
        "def get_facial_recog_features(input_img, cnn_model):\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  input = torch.zeros(1, 1, 128, 128)\n",
        "  img = input_img.numpy()\n",
        "\n",
        "  if img.shape[-1]!=1:\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  img = cv2.resize(img, (128, 128))\n",
        "  img = np.reshape(img, (128, 128, 1))\n",
        "  img = transform(img)\n",
        "  input[0,:,:,:] = img\n",
        "\n",
        "  input_var = torch.autograd.Variable(input, volatile=True)\n",
        "  _, features = cnn_model(input_var)\n",
        "  return features"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDny1cwZQPQ5"
      },
      "source": [
        "def keypoint_loss(input, output, kp_model):\n",
        "  ground_truth = kp_model.predict(input)\n",
        "  return tf.math.reduce_sum(tf.math.squared_difference(output, ground_truth))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x86nPyck4c4O"
      },
      "source": [
        "def aux_task_loss(input, output):\n",
        "  return tf.math.reduce_sum(tf.math.squared_difference(output, input))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVIEogHOn0F9"
      },
      "source": [
        "def loss_fn(input, output, cnn_model):\n",
        "  # make our own loss function considering \n",
        "  # run output thru head pose estimator and check difference from facing on\n",
        "  # feature distances between facial recognition on original image vs generated image\n",
        "  \n",
        "  input = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY)\n",
        "  loss = 0\n",
        "\n",
        "  output_lp = tfio.experimental.filter.laplacian(output, ksize=3)\n",
        "  lapl_loss = tf.math.reduce_sum(tf.math.squared_difference(output_lp, tf.reverse(output_lp, [2])))\n",
        "\n",
        "  sym_loss = tf.math.reduce_sum(tf.math.squared_difference(output, tf.reverse(output, [2])))\n",
        "\n",
        "  feature_loss = 0\n",
        "  for i in range(output.shape[0]):\n",
        "    output_features = get_facial_recog_features(output[i,:,:,:], cnn_model).detach().numpy()\n",
        "    input_features = get_facial_recog_features(input[i,:,:,:], cnn_model).detach().numpy()\n",
        "    feature_loss += tf.math.reduce_sum(tf.math.squared_difference(tf.convert_to_tensor(output_features), \n",
        "                                                                  tf.convert_to_tensor(input_features)))\n",
        "\n",
        "  loss = sym_loss + lapl_loss\n",
        "  loss /= tf.math.reduce_prod(tf.cast(tf.shape(output), tf.float32)) # N times height times width times channels\n",
        "  loss += feature_loss/tf.math.reduce_prod(tf.convert_to_tensor(output_features.shape))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Ppxjv6oKJj"
      },
      "source": [
        "#=======================PARAMETERS==============================================\n",
        "\n",
        "lr = 0.001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=lr,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "num_epochs = 80\n",
        "gamma1 = 0.33\n",
        "gamma2 = 0.33\n",
        "gamma3 = 0.33\n",
        "\n",
        "checkpoint_path = \"checkpoints/\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BdahMJGrcoC"
      },
      "source": [
        "def train(train_ds, num_epochs):\n",
        "  cnn_model = light_CNN()\n",
        "  print(model.summary())\n",
        "  min_loss = float('inf')\n",
        "\n",
        "  loss_value_1 = 0\n",
        "  loss_value_2 = 0\n",
        "  loss_value_3 = 0\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    print(\"Training epoch\", i)\n",
        "    for batch in tqdm(train_ds):\n",
        "      with tf.GradientTape() as tape:\n",
        "          batch = batch/255.0\n",
        "          # Forward pass.\n",
        "          if num_tasks==2:\n",
        "            rotated_output = first_task(batch)\n",
        "            reconstructed_output = model(batch)\n",
        "          else:\n",
        "            rotated_output, reconstructed_output, keypoint_output = model(batch)\n",
        "\n",
        "          # Loss value for this batch.\n",
        "          loss_value_1 = loss_fn(batch, rotated_output, cnn_model)\n",
        "          loss_value_2 = aux_task_loss(batch, reconstructed_output)\n",
        "          if num_tasks==3:\n",
        "            loss_value_3 = keypoint_loss(batch, keypoint_output, kp_model)\n",
        "          loss = gamma1*loss_value_1 + gamma2*loss_value_2 + gamma3*loss_value_3\n",
        "\n",
        "      # Get gradients of loss wrt the weights.\n",
        "      gradients = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "      # Update the weights of the model.\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "      if loss<min_loss:\n",
        "        model.save(checkpoint_path + \"best_model\")\n",
        "        first_task.save(checkpoint_path+\"best_first_task\")\n",
        "        min_loss = loss\n",
        "\n",
        "    if i%2==0:\n",
        "      model.save(checkpoint_path+\"epoch_\"+str(i))\n",
        "      first_task.save(checkpoint_path+\"first_task_epoch_\"+str(i))\n",
        "\n",
        "    if num_tasks==2:\n",
        "      print(\"Epoch {}, task 1 loss={}, task 2 loss={}, loss={}\".format(i, loss_value_1.numpy(), loss_value_2.numpy(), loss.numpy()))\n",
        "    else:\n",
        "      print(\"Epoch {}, task 1 loss={}, task 2 loss={}, task 3 loss={}, loss={}\".format(i, loss_value_1.numpy(), loss_value_2.numpy(), loss_value_3.numpy(), loss.numpy()))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOmW7MOLXnWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "580e4b979a834847a94df049ba31cd1a",
            "6ab277b2a0b44cc684305bbab65a936f",
            "268a2f4d4550412280a5402a3ca34b34",
            "530dabb2318f4ebda5a25779fc090c00",
            "4c1fb6e19a3c4e32839bbb356107373b",
            "f7479183ace64d7992fcd261b5726ff5",
            "bd181682e7364ed4a6a935c23ed6806c",
            "d43e6b0c72f34feeb29552ad5e40345e"
          ]
        },
        "outputId": "d53516fa-374f-4fb8-8e9c-a35fccdd9df5"
      },
      "source": [
        "train(train_ds, num_epochs)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "locally_connected2d_3 (Local (2, 54, 54, 32)           13810176  \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (2, 27, 27, 32)           0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (2, 1, 1, 23328)          0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (2, 1, 1, 3600)           83984400  \n",
            "_________________________________________________________________\n",
            "reshape_9 (Reshape)          (2, 60, 60, 1)            0         \n",
            "_________________________________________________________________\n",
            "locally_connected2d_4 (Local (2, 56, 56, 32)           2609152   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (2, 18, 18, 32)           0         \n",
            "_________________________________________________________________\n",
            "reshape_10 (Reshape)         (2, 1, 1, 10368)          0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (2, 1, 1, 3721)           38583049  \n",
            "_________________________________________________________________\n",
            "reshape_11 (Reshape)         (2, 61, 61, 1)            0         \n",
            "_________________________________________________________________\n",
            "reshape_12 (Reshape)         (2, 1, 1, 3721)           0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (2, 1, 1, 3600)           13399200  \n",
            "_________________________________________________________________\n",
            "reshape_13 (Reshape)         (2, 60, 60, 1)            0         \n",
            "_________________________________________________________________\n",
            "locally_connected2d_5 (Local (2, 56, 56, 32)           2609152   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (2, 18, 18, 32)           0         \n",
            "_________________________________________________________________\n",
            "reshape_14 (Reshape)         (2, 1, 1, 10368)          0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (2, 1, 1, 3721)           38583049  \n",
            "_________________________________________________________________\n",
            "reshape_15 (Reshape)         (2, 61, 61, 1)            0         \n",
            "=================================================================\n",
            "Total params: 193,578,178\n",
            "Trainable params: 193,578,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "580e4b979a834847a94df049ba31cd1a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8ab413533b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-1eee035a0e88>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_ds, num_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0;31m# Forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           \u001b[0mrotated_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoint_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0;31m# Loss value for this batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    }
  ]
}